{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch import nn \n",
    "\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CEM(nn.Module):\n",
    "    def __init__(self, state_dim, action_n):\n",
    "        super().__init__()\n",
    "        self.state_dim = state_dim\n",
    "        self.action_n = action_n\n",
    "        \n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, 100), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(100, self.action_n)\n",
    "        )\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.01)\n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def forward(self, _input):\n",
    "        return self.network(_input) \n",
    "    \n",
    "    def get_action(self, state):\n",
    "        state = torch.FloatTensor(np.array(state))\n",
    "        logits = self.forward(state)\n",
    "        action_prob = self.softmax(logits).detach().numpy()\n",
    "        action = np.random.choice(self.action_n, p=action_prob)\n",
    "        return action\n",
    "    \n",
    "    def update_policy(self, elite_trajectories):\n",
    "        elite_states = []\n",
    "        elite_actions = []\n",
    "        for trajectory in elite_trajectories:\n",
    "            elite_states.extend(trajectory['states'])\n",
    "            elite_actions.extend(trajectory['actions'])\n",
    "        elite_states = torch.FloatTensor(np.array(elite_states))\n",
    "        elite_actions = torch.LongTensor(np.array(elite_actions))\n",
    "        \n",
    "        loss = self.loss(self.forward(elite_states), elite_actions)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trajectory(env, agent, trajectory_len, visualize=False):\n",
    "    trajectory = {'states':[], 'actions': [], 'total_reward': 0}\n",
    "\n",
    "    state, _ = env.reset()\n",
    "\n",
    "    for _ in range(trajectory_len):\n",
    "        trajectory['states'].append(state)\n",
    "\n",
    "        action = agent.get_action(state)\n",
    "        trajectory['actions'].append(action)\n",
    "\n",
    "        state, reward, done, truncate, _ = env.step(action)\n",
    "\n",
    "        if done or truncate:\n",
    "            break\n",
    "\n",
    "        if visualize:\n",
    "            env.render()\n",
    "        trajectory['total_reward'] += reward\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_elite_trajectories(trajectories, q_param):\n",
    "    total_rewards = [trajectory['total_reward'] for trajectory in trajectories]\n",
    "    quantile = np.quantile(total_rewards, q=q_param) \n",
    "    return [trajectory for trajectory in trajectories if trajectory['total_reward'] > quantile]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MountainCar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Mountain Car](https://gymnasium.farama.org/_images/mountain_car.gif \"Mountain Car\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "*env.observation_space.shape, env.action_space.n    #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]  #type: ignore\n",
    "action_n = env.action_space.n   #type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = CEM(state_dim, action_n)\n",
    "episode_n = 200\n",
    "trajectory_n = 20\n",
    "trajectory_len = 500\n",
    "q_param = 0.8\n",
    "total_reward = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for episode in tqdm(range(episode_n)):\n",
    "    trajectories = [get_trajectory(env, agent, trajectory_len) for _ in range(trajectory_n)]\n",
    "    \n",
    "    mean_total_reward = np.mean([trajectory['total_reward'] for trajectory in trajectories])\n",
    "    total_reward.append(mean_total_reward)\n",
    "    \n",
    "    elite_trajectories = get_elite_trajectories(trajectories, q_param)\n",
    "    \n",
    "    if len(elite_trajectories) > 0:\n",
    "        agent.update_policy(elite_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(total_reward)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total reward')\n",
    "plt.title('Deep Cross Entropy Method')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "otus_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
